{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab913a",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2985b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c96f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vanilla\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706e2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat\\'s new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a08aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat\\'s new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]\n",
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback,\"), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\nLangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangGraphDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more\\n\\u200bAcknowledgements'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'), Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat's new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\")]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_5832\\2081252122.py:16: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=(OllamaEmbeddings(model=\"gemma:2b\"))\n"
     ]
    }
   ],
   "source": [
    "## iterating over the docs\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(docs_list)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(doc_splits)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=(OllamaEmbeddings(model=\"gemma:2b\"))\n",
    ")\n",
    "\n",
    "# vectorstore=FAISS.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     embedding=OpenAIEmbeddings()\n",
    "# )\n",
    "\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea4cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a216c62f-f3a8-4871-a7bb-be8fb40696dd', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents‚Äî including Klarna, Replit, Elastic, and more‚Äî LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don‚Äôt need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain‚Äôs agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopypip install -U langgraph'),\n",
       " Document(id='8ed880dd-ce17-483c-bbb4-ad5346d98759', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat's new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\"),\n",
       " Document(id='e54da34d-d351-41a2-9251-70efb5ee7a80', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.'),\n",
       " Document(id='64245e8c-0a58-45cd-87ce-e03a4e929a5c', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Then, create a simple hello world example:\\nCopyfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a7a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever To Retriever Tools to integrate with llms\n",
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langgraph\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0712cf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002B5EDD6BEB0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002B5E2E275B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f946b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check what's in langchain.tools\n",
    "# import langchain.tools\n",
    "# print(\"Contents of langchain.tools:\")\n",
    "# print(dir(langchain.tools))\n",
    "\n",
    "# # Check if there's a tools module at all\n",
    "# import langchain\n",
    "# print(\"\\nLangChain version:\", langchain.__version__)\n",
    "\n",
    "# # List all submodules\n",
    "# print(\"\\nAvailable submodules:\")\n",
    "# import pkgutil\n",
    "# for importer, modname, ispkg in pkgutil.iter_modules(langchain.__path__):\n",
    "#     print(f\"  {modname} {'(package)' if ispkg else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002B5EDD6BEB0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002B5E2E275B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac6e08",
   "metadata": {},
   "source": [
    "### Langchain Blogs- Seperate Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2ed2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrocküëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrocküëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrocküëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')],\n",
       " [Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentüöÄ Share how you\\'re building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewChangelogGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopypip install -U langchain\\n# Requires Python 3.10+\\n\\n\\u200b Create an agent\\nCopy# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain‚Äôs agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain‚Äôs agents are built on top of LangGraph. This allows us to take advantage of LangGraph‚Äôs durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoChangelogNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f834d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=(OllamaEmbeddings(model=\"gemma:2b\"))\n",
    ")\n",
    "\n",
    "# vectorstorelangchain=FAISS.from_documents(\n",
    "#     documents=doc_splits,\n",
    "#     embedding=OpenAIEmbeddings()\n",
    "# )\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf70c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edbc7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine these two tools we created\n",
    "tools=[retriever_tool,retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8908e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002B5EDD6BEB0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002B5E2E275B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1D240>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " Tool(name='retriever_vector_langchain_blog', description='Search and run information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002B5EDD6BEB0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1FBB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002B5E2E275B0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B5E2E1FBB0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a65ca",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127c309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b581a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'reasoning_content': 'The user just says \"Hi\". We need to respond politely. Probably ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 72, 'total_tokens': 110, 'completion_time': 0.079648236, 'prompt_time': 0.002783782, 'queue_time': 0.054229438, 'total_time': 0.082432018, 'completion_tokens_details': {'reasoning_tokens': 20}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--92aba4fb-c180-4dd2-aa1d-c40bd3975b80-0', usage_metadata={'input_tokens': 72, 'output_tokens': 38, 'total_tokens': 110, 'output_token_details': {'reasoning': 20}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5176f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67749188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed80d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "884aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "client = Client()\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate proper answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    docs = messages[-1].content\n",
    "\n",
    "    # Pull prompt correctly\n",
    "    prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "    llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c284c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Client', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_types', 'client']\n"
     ]
    }
   ],
   "source": [
    "import langchainhub\n",
    "import inspect\n",
    "print(dir(langchainhub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa2a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26bbd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "661decb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='ee81c4b8-629e-4b11-a23c-a6cc4c992281'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user asks: \"What is Langgraph?\" We need to answer. Possibly we should retrieve info from a blog about Langgraph. Use the function retriever_vector_db_blog with query \"Langgraph\".', 'tool_calls': [{'id': 'fc_0580be06-9b6c-42e8-b050-32a6b6ec2e9d', 'function': {'arguments': '{\"query\":\"Langgraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 172, 'total_tokens': 246, 'completion_time': 0.154448694, 'prompt_time': 0.006880424, 'queue_time': 0.048107426, 'total_time': 0.161329118, 'completion_tokens_details': {'reasoning_tokens': 42}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_1d1727abc9', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--af23e275-ebbd-4c4a-8dbd-531f0b7c5907-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'Langgraph'}, 'id': 'fc_0580be06-9b6c-42e8-b050-32a6b6ec2e9d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 172, 'output_tokens': 74, 'total_tokens': 246, 'output_token_details': {'reasoning': 42}}),\n",
       "  ToolMessage(content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat's new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\", name='retriever_vector_db_blog', id='a57fc51f-e6ff-4a5e-a9d1-4bb0cdf9f48b', tool_call_id='fc_0580be06-9b6c-42e8-b050-32a6b6ec2e9d'),\n",
       "  HumanMessage(content='LangGraph is a framework (part of the LangChain ecosystem) for building stateful, graph‚Äëbased applications that orchestrate large language model (LLM) calls. It lets developers define nodes and edges to represent complex LLM workflows, enabling modular, reusable, and observable pipelines. LangGraph provides tools for programmatic integration with services like Claude, VS\\u202fCode, and other tools via its MCP interface.', additional_kwargs={}, response_metadata={}, id='246f6385-5fbb-456f-adcb-958f8f1c833c')]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph.invoke({\"messages\":\"What is Langgraph?\"})\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"What is Langgraph?\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "448a376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Langgraph', additional_kwargs={}, response_metadata={}, id='fff5d574-c44e-4637-afd6-7105ab7ab08b'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user just says \"Langgraph\". Likely they want information about Langgraph. We should retrieve relevant info. Use retriever_vector_db_blog with query \"Langgraph\".', 'tool_calls': [{'id': 'fc_330cf778-8a73-49f0-b635-4a65e53b52c3', 'function': {'arguments': '{\"query\":\"Langgraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 169, 'total_tokens': 237, 'completion_time': 0.14277723, 'prompt_time': 0.007232976, 'queue_time': 0.048139784, 'total_time': 0.150010206, 'completion_tokens_details': {'reasoning_tokens': 36}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--e0b4eddb-1798-4999-a0fc-99c8a3acbd08-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'Langgraph'}, 'id': 'fc_330cf778-8a73-49f0-b635-4a65e53b52c3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 169, 'output_tokens': 68, 'total_tokens': 237, 'output_token_details': {'reasoning': 36}}),\n",
       "  ToolMessage(content=\"Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWhat's new in LangGraph v1Next‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\", name='retriever_vector_db_blog', id='9cc5f1ec-2e8b-4d39-977a-e46648286abc', tool_call_id='fc_330cf778-8a73-49f0-b635-4a65e53b52c3'),\n",
       "  HumanMessage(content='I‚Äôm sorry, but I don‚Äôt have enough information to answer that question.', additional_kwargs={}, response_metadata={}, id='a8a189b4-6acc-4d4a-814f-e8cf33d38012')]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"Langgraph\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3e9d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Langchain', additional_kwargs={}, response_metadata={}, id='3cd4a9c3-3202-4b83-9159-dc7d178689f3'),\n",
       "  AIMessage(content='Sure! Here‚Äôs a quick snapshot of **LangChain**‚Äîwhat it is, why it matters, and how you can start using it. Let me know if you‚Äôd like deeper details on any particular area (e.g., code examples, deployment, integrations, or a comparison with LangGraph).\\n\\n---\\n\\n## 1Ô∏è‚É£ What is LangChain?\\n\\n**LangChain** is an open‚Äësource framework that makes it easier to build **applications powered by large language models (LLMs)**. It does this by providing modular, reusable building blocks for:\\n\\n| Layer | What it does | Typical components |\\n|------|--------------|--------------------|\\n| **LLM** | Calls to any LLM provider (OpenAI, Anthropic, Cohere, etc.) | `OpenAI`, `ChatOpenAI`, `Anthropic`, `Cohere` wrappers |\\n| **Prompt** | Structured prompt creation & templating | `PromptTemplate`, `FewShotPromptTemplate`, `ChatPromptTemplate` |\\n| **Memory** | Persists context across turns (chat, agents) | `ConversationBufferMemory`, `VectorStoreRetrieverMemory`, `SummaryMemory` |\\n| **Chains** | Sequential or parallel composition of LLM calls, prompts, and utilities | `LLMChain`, `SequentialChain`, `RouterChain`, `TransformChain` |\\n| **Agents** | Dynamic decision‚Äëmaking: choose tools, run actions, loop until a goal is met | `ZeroShotAgent`, `ToolCallingAgent`, `ReactAgent`, `OpenAIFunctionsAgent` |\\n| **Tools** | External APIs or utilities that agents can invoke (search, DB, calculators, etc.) | `SerpAPI`, `SQLDatabaseTool`, `PythonREPL`, custom HTTP tools |\\n| **Retrievers / Vector Stores** | Ground LLM output in external knowledge (RAG) | `FAISS`, `Pinecone`, `Weaviate`, `Chroma`, `Milvus` |\\n| **Evaluation** | Automated testing of LLM outputs | `LLMEvalChain`, `ChatEvalChain`, custom metrics |\\n| **Callbacks** | Hooks for logging, tracing, streaming, UI integration | `ConsoleCallbackHandler`, `LangChainTracer`, `OpenTelemetry` |\\n\\nAll of these pieces are **plug‚Äëand‚Äëplay**, so you can mix‚Äëand‚Äëmatch to create anything from a simple Q&A bot to a sophisticated autonomous agent.\\n\\n---\\n\\n## 2Ô∏è‚É£ Why Use LangChain?\\n\\n| Benefit | Explanation |\\n|--------|-------------|\\n| **Productivity** ‚Äì No need to reinvent prompt handling, memory, or tool‚Äëcalling logic. |\\n| **Portability** ‚Äì Same code works with any LLM provider (OpenAI, Azure, HuggingFace, Anthropic, etc.). |\\n| **Extensibility** ‚Äì Easy to add custom tools, memory stores, or evaluation metrics. |\\n| **Community & Ecosystem** ‚Äì Rich set of integrations (vector DBs, APIs, UI frameworks) and a vibrant open‚Äësource community. |\\n| **Observability** ‚Äì Built‚Äëin callbacks for tracing, debugging, and monitoring. |\\n\\n---\\n\\n## 3Ô∏è‚É£ Core Concepts & Quick Code Sketch\\n\\nBelow is a **minimal ‚Äúchat with memory‚Äù** example using Python. It demonstrates the most common pieces: LLM, prompt template, memory, and a chain.\\n\\n```python\\n# Install first\\n# pip install langchain openai\\n\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.chains import LLMChain\\n\\n# 1Ô∏è‚É£ LLM wrapper (uses your OPENAI_API_KEY)\\nllm = OpenAI(model=\"gpt-4o-mini\", temperature=0.7)\\n\\n# 2Ô∏è‚É£ Prompt template (you can also use ChatPromptTemplate for chat messages)\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant that remembers the conversation.\"),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n])\\n\\n# 3Ô∏è‚É£ Memory ‚Äì keeps the whole conversation\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\\n\\n# 4Ô∏è‚É£ Chain ‚Äì ties everything together\\nchat_chain = LLMChain(\\n    llm=llm,\\n    prompt=prompt,\\n    memory=memory,\\n    verbose=True  # prints intermediate steps\\n)\\n\\n# 5Ô∏è‚É£ Interact\\nwhile True:\\n    user_input = input(\"\\\\nYou: \")\\n    if user_input.lower() in {\"exit\", \"quit\"}:\\n        break\\n    answer = chat_chain.run(question=user_input)\\n    print(\"\\\\nAssistant:\", answer)\\n```\\n\\n**What happens under the hood?**\\n\\n1. Your input is rendered into the prompt template.\\n2. The `ConversationBufferMemory` appends the new user message and the previous assistant response to the prompt, giving the LLM full context.\\n3. The `LLMChain` calls the LLM, returns the answer, and stores it back in memory.\\n\\n---\\n\\n## 4Ô∏è‚É£ Building More Advanced Apps\\n\\n### a. Retrieval‚ÄëAugmented Generation (RAG)\\n\\n```python\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import RetrievalQA\\n\\n# 1Ô∏è‚É£ Create a vector store from your docs\\ndocs = [...]  # list of LangChain Document objects\\nvectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\\n\\n# 2Ô∏è‚É£ Retriever\\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\\n\\n# 3Ô∏è‚É£ QA chain\\nqa = RetrievalQA.from_chain_type(\\n    llm=OpenAI(model=\"gpt-4o-mini\"),\\n    chain_type=\"stuff\",   # can be \"map_reduce\", \"refine\", etc.\\n    retriever=retriever,\\n    return_source_documents=True,\\n)\\n\\n# 4Ô∏è‚É£ Ask a question\\nresult = qa({\"query\": \"How does LangChain handle tool calling?\"})\\nprint(result[\"answer\"])\\nprint(\"\\\\nSources:\")\\nfor doc in result[\"source_documents\"]:\\n    print(\"-\", doc.metadata.get(\"source\"))\\n```\\n\\n### b. Tool‚ÄëCalling Agents (React/Function‚ÄëCalling)\\n\\n```python\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.tools import SerpAPIWrapper, PythonREPLTool\\n\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=SerpAPIWrapper().run,\\n        description=\"Useful for searching the web for up‚Äëto‚Äëdate facts.\"\\n    ),\\n    Tool(\\n        name=\"Python REPL\",\\n        func=PythonREPLTool().run,\\n        description=\"Executes arbitrary Python code.\"\\n    ),\\n]\\n\\nagent = initialize_agent(\\n    tools,\\n    OpenAI(model=\"gpt-4o-mini\", temperature=0),\\n    agent=\"zero-shot-react-description\",\\n    verbose=True,\\n)\\n\\nagent.run(\"What was the closing price of AAPL on 2024‚Äë10‚Äë01 and plot it with a simple line chart.\")\\n```\\n\\nThe agent decides whether to **search**, **run Python**, or **answer directly**, looping until it reaches a final answer.\\n\\n---\\n\\n## 5Ô∏è‚É£ LangChain vs. LangGraph\\n\\n| Feature | LangChain | LangGraph |\\n|---------|-----------|-----------|\\n| **Primary focus** | Building *chains* and *agents* (sequential or branching). | Designing *state‚Äëful, graph‚Äëbased workflows* (think ‚Äúworkflow engine‚Äù). |\\n| **Abstractions** | Chains, Agents, Memory, Retrievers, Tools. | Nodes, Edges, State, Conditional logic, Loops. |\\n| **When to use** | Quick prototypes, RAG, tool‚Äëcalling bots, simple or moderately complex pipelines. | Complex multi‚Äëstep orchestration, long‚Äërunning processes, need explicit state tracking and branching (e.g., multi‚Äëstage data pipelines, multi‚Äëagent coordination). |\\n| **Interoperability** | Fully compatible ‚Äì you can embed a LangChain *Chain* or *Agent* as a node inside a LangGraph graph. | Uses LangChain components under the hood; you can swap them in/out. |\\n\\nIf you start with a simple chatbot, stick with **LangChain**. When your application grows into a multi‚Äëstage workflow (e.g., ‚Äúfetch data ‚Üí summarize ‚Üí generate report ‚Üí send email‚Äù), consider moving that logic into **LangGraph** for clearer state handling.\\n\\n---\\n\\n## 6Ô∏è‚É£ Getting Started & Resources\\n\\n| Resource | Link |\\n|----------|------|\\n| **Official Docs** | <https://python.langchain.com> |\\n| **Quick‚ÄëStart Notebook** | <https://github.com/langchain-ai/langchain/blob/master/docs/tutorials/quickstart.ipynb> |\\n| **Community Slack** | <https://langchain.slack.com> (invite via the website) |\\n| **GitHub Repo** | <https://github.com/langchain-ai/langchain> |\\n| **Video Walkthroughs** | LangChain YouTube channel ‚Äì ‚ÄúBuilding a Retrieval‚ÄëAugmented Chatbot‚Äù |\\n| **Blog Posts** | ‚ÄúLangChain 0.2 Release Highlights‚Äù (Oct\\u202f2024) ‚Äì covers new callbacks & async support. |\\n\\n---\\n\\n## 7Ô∏è‚É£ What Next?\\n\\n- **Pick a use‚Äëcase**: Q&A bot, data extraction, autonomous agent, etc.\\n- **Choose a vector store** if you need RAG (FAISS for local dev, Pinecone/Weaviate for production).\\n- **Add memory** if you need multi‚Äëturn context.\\n- **Iterate**: start with a simple `LLMChain`, then layer in retrieval, tools, and finally an agent or a LangGraph workflow.\\n\\n---\\n\\n### üéØ Quick Checklist for a Minimal Production‚ÄëReady Bot\\n\\n1. **Environment** ‚Äì Secure API keys (use `.env` + `python-dotenv`).\\n2. **LLM Provider** ‚Äì Choose a model with the right cost/latency trade‚Äëoff.\\n3. **Prompt Management** ‚Äì Keep prompts in separate files or a prompt‚Äëstore for versioning.\\n4. **Vector Store** ‚Äì Persist embeddings; set up periodic re‚Äëindexing if docs change.\\n5. **Memory** ‚Äì Use a durable store (Redis, DynamoDB) for long‚Äëterm sessions.\\n6. **Observability** ‚Äì Enable LangChain callbacks ‚Üí log to your monitoring stack.\\n7. **Safety** ‚Äì Add content filters or guardrails (OpenAI moderation, custom heuristics).\\n8. **Testing** ‚Äì Write unit tests with `LangChainEvalChain` or mock LLM responses.\\n\\n---\\n\\nLet me know which part you‚Äôd like to dive deeper into‚Äîcode samples, deployment tips, advanced agent patterns, LangGraph integration, or anything else! üöÄ', additional_kwargs={'reasoning_content': 'The user just says \"Langchain\". Likely they want information about Langchain. Could be a request for an overview, explanation, usage, comparison with Langgraph, etc. Since no explicit question, we need to respond helpfully: provide overview of Langchain, its components, use cases, recent updates, maybe mention Langgraph integration. Could also ask clarification. Probably best to ask what they need: overview, tutorial, etc. But we can give a concise intro and ask follow-up.'}, response_metadata={'token_usage': {'completion_tokens': 2321, 'prompt_tokens': 169, 'total_tokens': 2490, 'completion_time': 4.899753781, 'prompt_time': 0.00644459, 'queue_time': 0.049200229, 'total_time': 4.906198371, 'completion_tokens_details': {'reasoning_tokens': 101}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--53f658a8-3aaf-4901-a355-42c456fb06ba-0', usage_metadata={'input_tokens': 169, 'output_tokens': 2321, 'total_tokens': 2490, 'output_token_details': {'reasoning': 101}})]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"Langchain\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03cb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='9e30ea14-198e-4d66-a99d-0e92e56cf200'),\n",
       "  AIMessage(content='**Machine learning (ML)** is a sub‚Äëfield of artificial intelligence (AI) that focuses on building systems that can **learn from data** and improve their performance on a task without being explicitly programmed for every possible situation.\\n\\n### Core Idea\\n- **Traditional programming:** You write explicit rules (if‚Äëelse statements) that tell a computer exactly what to do.\\n- **Machine learning:** You give the computer **examples (data)** and a **goal** (e.g., predict a label, generate a recommendation). The computer automatically discovers patterns and creates a model that can make predictions or decisions on new, unseen data.\\n\\n### How It Works (High‚ÄëLevel Pipeline)\\n\\n| Step | Description |\\n|------|-------------|\\n| 1Ô∏è‚É£ **Define the problem** | Classification, regression, clustering, recommendation, generation, etc. |\\n| 2Ô∏è‚É£ **Collect data** | Gather relevant, representative data (images, text, sensor readings, etc.). |\\n| 3Ô∏è‚É£ **Prepare data** | Clean, label, normalize, split into training/validation/test sets. |\\n| 4Ô∏è‚É£ **Choose a model** | Linear models, decision trees, neural networks, support‚Äëvector machines, etc. |\\n| 5Ô∏è‚É£ **Train the model** | Feed the training data to the algorithm; it adjusts internal parameters to minimize a loss function. |\\n| 6Ô∏è‚É£ **Validate & tune** | Evaluate on validation data, adjust hyper‚Äëparameters, prevent over‚Äë/under‚Äëfitting. |\\n| 7Ô∏è‚É£ **Test & deploy** | Measure final performance on a held‚Äëout test set, then integrate into an application. |\\n| 8Ô∏è‚É£ **Monitor & update** | Continuously track performance; retrain with new data as needed. |\\n\\n### Main Types of Machine Learning\\n\\n| Category | Goal | Typical Algorithms | Example Use‚ÄëCases |\\n|----------|------|--------------------|------------------|\\n| **Supervised Learning** | Learn a mapping from inputs ‚Üí outputs using labeled data. | Linear/Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, Neural Networks (CNNs, RNNs) | Spam detection, image classification, house‚Äëprice prediction |\\n| **Unsupervised Learning** | Discover hidden structure in unlabeled data. | K‚Äëmeans, Hierarchical Clustering, PCA, Autoencoders, GANs (for generation) | Customer segmentation, anomaly detection, dimensionality reduction |\\n| **Semi‚ÄëSupervised Learning** | Combine a small amount of labeled data with a large amount of unlabeled data. | Self‚Äëtraining, Graph‚Äëbased methods, Consistency regularization | Medical imaging where labeling is expensive |\\n| **Reinforcement Learning (RL)** | Learn a policy to maximize cumulative reward through interaction with an environment. | Q‚Äëlearning, Deep Q‚ÄëNetwork (DQN), Proximal Policy Optimization (PPO) | Game playing (AlphaGo), robotics, recommendation systems that adapt over time |\\n| **Self‚ÄëSupervised Learning** | Create supervisory signals from the data itself (often used in large language and vision models). | Contrastive learning, masked language modeling (BERT), vision transformers | Pre‚Äëtraining large models like GPT‚Äë4, CLIP, DALL¬∑E |\\n\\n### Key Concepts & Terminology\\n\\n- **Model:** The mathematical function (e.g., a neural network) that maps inputs to outputs.\\n- **Parameters:** Values the model learns during training (weights, biases).\\n- **Hyper‚Äëparameters:** Settings you choose before training (learning rate, number of layers, regularization strength).\\n- **Loss / Cost Function:** Quantifies how far predictions are from true values; training aims to minimize it.\\n- **Overfitting / Underfitting:** Overfitting = model memorizes training data and fails on new data; underfitting = model is too simple to capture patterns.\\n- **Feature Engineering:** Transforming raw data into informative inputs for the model (e.g., scaling, encoding categorical variables, extracting text n‚Äëgrams).\\n- **Evaluation Metrics:** Accuracy, precision/recall, F1‚Äëscore, ROC‚ÄëAUC (classification); RMSE, MAE (regression); silhouette score (clustering).\\n\\n### Popular Tools & Frameworks\\n\\n| Domain | Libraries / Platforms |\\n|--------|-----------------------|\\n| General ML / Data preprocessing | **scikit‚Äëlearn**, **pandas**, **NumPy** |\\n| Deep learning | **TensorFlow**, **PyTorch**, **Keras**, **JAX** |\\n| AutoML | **Auto‚Äësklearn**, **TPOT**, **Google Cloud AutoML**, **H2O.ai** |\\n| Model serving | **TensorFlow Serving**, **TorchServe**, **FastAPI**, **MLflow**, **Docker/Kubernetes** |\\n| Experiment tracking | **Weights & Biases**, **MLflow**, **Neptune.ai** |\\n\\n### Real‚ÄëWorld Applications\\n\\n| Sector | Example |\\n|--------|---------|\\n| **Healthcare** | Disease diagnosis from medical images, drug discovery, patient risk stratification |\\n| **Finance** | Credit scoring, fraud detection, algorithmic trading |\\n| **Retail & E‚Äëcommerce** | Product recommendation, demand forecasting, dynamic pricing |\\n| **Transportation** | Autonomous driving perception, route optimization, predictive maintenance |\\n| **Entertainment** | Content recommendation (Netflix, Spotify), video game AI, music generation |\\n| **Manufacturing** | Quality inspection via computer vision, predictive equipment failure |\\n| **Natural Language Processing** | Chatbots, sentiment analysis, machine translation, code generation (e.g., GitHub Copilot) |\\n\\n### Why Machine Learning Matters\\n\\n1. **Scalability:** Automates tasks that would be impossible or too costly for humans to code manually.\\n2. **Adaptability:** Models can be retrained with new data, allowing systems to evolve as the world changes.\\n3. **Insight Discovery:** Uncovers hidden patterns, correlations, and predictive signals that may not be obvious to domain experts.\\n4. **Economic Impact:** Drives efficiency, creates new products/services, and opens up entirely new business models.\\n\\n### Getting Started (If You‚Äôre New)\\n\\n1. **Learn the basics of Python** (the de‚Äëfacto language for ML) and libraries like **pandas** and **NumPy**.\\n2. **Study fundamental concepts**: linear regression, logistic regression, gradient descent, overfitting, cross‚Äëvalidation.\\n3. **Complete a hands‚Äëon project**: e.g., Kaggle‚Äôs ‚ÄúTitanic‚Äù classification or a simple house‚Äëprice regression using scikit‚Äëlearn.\\n4. **Explore deep learning** with a tutorial on building a basic image classifier in PyTorch or TensorFlow.\\n5. **Read and practice**: ‚ÄúHands‚ÄëOn Machine Learning with Scikit‚ÄëLearn, Keras, and TensorFlow‚Äù (Aur√©lien G√©ron) is an excellent beginner‚Äëto‚Äëintermediate guide.\\n6. **Join communities**: Kaggle, Stack Overflow, r/MachineLearning, and AI conferences (NeurIPS, ICML) for staying current.\\n\\n---\\n\\n**In a nutshell:** Machine learning equips computers with the ability to **learn patterns from data** and make predictions or decisions, enabling a wide array of intelligent applications across virtually every industry.', additional_kwargs={'reasoning_content': 'User asks \"What is Machine learning?\" Simple definition. Provide explanation, types, examples, steps, applications. Probably no need to use tools.'}, response_metadata={'token_usage': {'completion_tokens': 1518, 'prompt_tokens': 172, 'total_tokens': 1690, 'completion_time': 3.182456217, 'prompt_time': 0.007161532, 'queue_time': 0.048825777, 'total_time': 3.189617749, 'completion_tokens_details': {'reasoning_tokens': 30}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_1d1727abc9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--77b31257-08bd-4567-aa0f-e82c98871832-0', usage_metadata={'input_tokens': 172, 'output_tokens': 1518, 'total_tokens': 1690, 'output_token_details': {'reasoning': 30}})]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2427a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
